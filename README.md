# WWW 2021 Tutorial, Ljubljana, Slovenia

**Date:** April 14, 2021

**Start Time:** 2pm CEST/Ljubljana, 8am EDT, 5am PDT

**End Time:** 6pm CEST/Ljubljana, noon EDT, 9am PDT

**Location:** Virtual, [MiTeam > Tutorials > Learning From Comparisons](https://theweb.miteam.eu/asset/kF6frzSt4t9rKNoZj)

**Slides:** <a href="Learning%20from%20Comparisons%20-%20WWW%202021%20Tutorial.pdf.pdf">PDF</a>


-----


 1. [Presenters](#presenters)
 2. [Motivation and Summary](#motivation-and-summary)
 3. [Outline](#outline)
 4. [Schedule](#schedule)
 5. [Presenter Biographies](#presenter-biographies)
 6. [Acknowledgements](#acknowledgements)
 7. [References](#references)

-----


## Presenters

| <img src="images/jenniferdy.jpg" width="160"/>  | <img src="images/stratisioannidis.jpg" width="160"/>  | <img src="images/ilkayyildiz.jpg" width="154"/> |

[Jennifer Dy](https://ece.northeastern.edu/fac-ece/jdy/), Northeastern University, Electrical and Computer Engineering Department

[Stratis Ioannidis](https://ece.northeastern.edu/fac-ece/ioannidis/), Northeastern University, Electrical and Computer Engineering Department

[Ilkay Yildiz](https://www.linkedin.com/in/ilkay-y%C4%B1ld%C4%B1z/), Northeastern University, Electrical and Computer Engineering Department


## Motivation and Summary 

Class labels generated by humans are often noisy, as data collected from multiple experts exhibit inconsistencies across labelers. To ameliorate this effect, one approach is to ask labelers to compare or rank samples instead: when class labels are ordered, a labeler presented with two or more samples can rank them w.r.t. their relative order, as induced by class membership. Comparisons are more informative than class labels, as they capture both inter- and intra-class relationships; the latter are not revealed by class labels alone. In addition, comparison labels are subject to reduced variability in practice: this has been experimentally observed in many domains, and is due to the fact that humans often find it easier to make relative‚Äìrather than absolute‚Äìjudgements. 

Nevertheless, learning from comparisons poses computational challenges regressing rankings features is a computationally intensive task. Learning from pairs of comparisons between ùëÅ samples corresponds to inference over ùëÇ(ùëÅ^2) comparison labels. More generally, learning from rankings of sample subsets of size K corresponds to inference over ùëÇ(ùëÅ^K) labels. This requires significantly improving the performance of, e.g., maximum likelihood estimation (MLE) algorithms over such datasets. Finally, collecting rankings is also labor intensive. This is precisely because of the ùëÇ(ùëÅ^K) size of the space of potential sets of size K to be labeled.  

The tutorial will review classic and recent approaches to tackle the problem of learning from comparisons and, more broadly, learning from ranked data. Particular focus will be paid to the ranking regression setting, whereby rankings are to be regressed from sample features. 

## Outline
- Parametric models: Bradley-Terry, Plackett-Luce, Thurstone. 
- Non-parametric Models:  noisy-permutation model, Mallows model, matrix factorization methods. 
- Maximum Likelihood Estimation and spectral algorithms. 
- Ranking regression and variational inference methods applied to comparisons. 
- Sample complexity guarantees for ranking regression. 
- Deep neural network models and accelerated learning methods. 
- Active learning from comparisons.

## Schedule


| **CEST** | **EDT** | **PDT**   | **Content** | **Duration** |
| :------| :----- |:--- | :----------- | :--------|
| 2:00pm | 8:00am | 5:00am  | Parametric models: Bradley-Terry, Plackett-Luce, Thurstone.  Maximum Likelihood Estimation and spectral algorithms. Non-parametric Models: noisy-permutation model, Mallows model. | 1h 15mins |
| 3:15pm | 9:15am | 6:15am    | Q&A + break | 15 mins |
| 3.30pm | 9:30am | 6:30am | Ranking regression. Deep neural network models and accelerated learning methods. | 1h 30mins |
| 5:00pm | 11:00am | 8:00am | Q&A + break | 15 mins |
| 5.15pm | 11:15am | 8:15am  | Variational inference methods applied to comparisons. Active learning from comparisons. Sample complexity. | 30mins |
| 5.45pm | 11:45am | 8.45am    | Q&A + final discussion | 15 mins |

## Presenter Biographies

**[Jennifer Dy](https://ece.northeastern.edu/fac-ece/jdy/)** is a Professor at the Department of Electrical and Computer Engineering, Northeastern University, Boston, MA, where she first joined the faculty in 2002. She received her M.S. and Ph.D. in 1997 and 2001 respectively from the School of Electrical and Computer Engineering, Purdue University, West Lafayette, IN, and her B.S. degree from the Department of Electrical Engineering, University of the Philippines, in 1993.  Her research spans both fundamental research in machine learning and their application to biomedical imaging, health, science and engineering, with research contributions in unsupervised learning, interpretable models, dimensionality reduction, feature selection/sparse methods, learning from uncertain experts, active learning, Bayesian models, and deep representations. She received an NSF Career award in 2004. She has served or is serving as Secretary for the International Machine Learning Society, associate editor/editorial board member for the Journal of Machine Learning Research, Machine Learning journal, IEEE Transactions on Pattern Analysis and Machine Intelligence, organizing and or technical program committee member for premier conferences in machine learning and data mining (ICML, NeurIPS, ACM SIGKDD, AAAI, IJCAI, UAI, AISTATS, ICLR, SIAM SDM), and program co-chair for SIAM SDM 2013 and ICML 2018.

**[Stratis Ioannidis](https://ece.northeastern.edu/fac-ece/ioannidis/)** is an Associate Professor in the Electrical and Computer Engineering Department of Northeastern University, in Boston, MA, where he also holds a courtesy appointment with the College of Computer and Information Science. His research interests span machine learning, distributed systems, networking, optimization, and privacy. He received his B.Sc. (2002) in Electrical and Computer Engineering from the National Technical University of Athens, Greece, and his M.Sc. (2004) and Ph.D. (2009) in Computer Science from the University of Toronto, Canada. Prior to joining Northeastern, he was a research scientist at the Technicolor research centers in Paris, France, and Palo Alto, CA, as well as at Yahoo Labs in Sunnyvale, CA. He is the recipient of an NSF CAREER Award, a Google Faculty Research Award, a  Facebook Research Award, and a best paper award at ACM ICN 2017 and IEEE DySPAN 2019. 

**[Ilkay Yildiz](https://www.linkedin.com/in/ilkay-y%C4%B1ld%C4%B1z/)** is a senior PhD student at the Department of Electrical and Computer Engineering, Northeastern University, Boston, MA. She received her undergraduate degree in Electrical and Electronics Engineering at Bilkent University, Ankara, Turkey in 2017. Her research interests span ranking and preference learning, deep learning, computer vision, probabilistic modeling, and optimization. Her research contributions involve accelerated regression algorithms that learn from choice and ranking labels.


## Acknowledgements

Our work is supported by NIH (R01EY019474), NSF (SCH-1622542 at MGH, SCH-1622536 at Northeastern, SCH-1622679 at OHSU, Facebook Statistics Fellowship, and by unrestricted departmental funding from Research to Prevent Blindness (OHSU).

[<img src="images/neu.png" alt="Northeastern University" width="50px"/>](https://ece.northeastern.edu/)
[<img src="images/mgh.png" alt="Massachusetts General Hospital" width="50px"/>](https://www.massgeneral.org/)
[<img src="images/ohsu.jpg" alt="Oregon Health and Science Institute" width="50px"/>](https://www.ohsu.edu/casey-eye-institute)
[<img src="images/nih.png" alt="NIH" width="50px"/>](https://www.nih.gov/)
[<img src="images/nsf.jpg" alt="NSF" width="50px"/>](https://www.nsf.gov/)
<img src="images/facebook.png" alt="Facebook" width="50px"/>


## References

1.	N. Ahmed and M. Campbell. Variational bayesian data fusion of multi-class discrete observations with applications to cooperative human-robot estimation. In ICRA, 2010.

2.	Ammar, A. and Shah, D. (2011). Ranking: Compare, don‚Äôt score. In 2011 49th Annual Allerton Conference on Communication, Control, and Computing (Allerton), pages 776‚Äì783. IEEE.

3.	Azari, H., Parks, D., and Xia, L. (2012). Random utility theory for social choice. In Advances in Neural Information Processing Systems, pages 126‚Äì134.

4.	Bosman, A. S., Engelbrecht, A., and Helbig, M. (2020). Visualising basins of attraction for the cross-entropy and the squared error neural network loss functions. Neurocomputing.

5.	G. Bouchard. Efficient bounds for the softmax function, applications to inference in hybrid models. In Presentation at the Workshop at NIPS, 2007.

6.	Boyd, S., Parikh, N., Chu, E., Peleato, B., and Eckstein, J. (2011). Distributed optimization and statistical learning via the alternating direction method of multipliers. Foundations and Trends¬Æ in Machine Learning, 3(1):1‚Äì122.

7.	Bradley, R. A. and Terry, M. E. (1952). Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324‚Äì345.

8.	Braverman, M. and Mossel, E. (2008). Noisy sorting without resampling. In Proceedings of the 19th Annual ACM-SIAM Symposium on Discrete Algorithms, pages 268‚Äì276. Society for Industrial and Applied Mathematics.

9.	J. P. Campbell, J. Kalpathy-Cramer, D. Erdoƒümu≈ü, P. Tian, D. Kedarisetti, C. Moleta, J. D. Reynolds, K. Hutcheson, M. J. Shapiro, M. X. Repka et al., ‚ÄúPlus disease in retinopathy of prematurity: A continuous spectrum of vascular abnormality as a basis of diagnostic variability,‚Äù Ophthalmology, vol. 123, no. 11, pp. 2338‚Äì2344, 2016.

10.	Caron, F. and Doucet, A. (2012). Efficient bayesian inference for generalized bradley‚Äìterry models. Journal of Computational and Graphical Statistics, 21(1):174‚Äì196.

11.	Caruana, R. and Niculescu-Mizil, A. (2004). Data mining in metric space: an empirical analysis of supervised learning performance criteria. In Proceedings of the tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 69‚Äì78.

12.	Cattelan, M. (2012). Models for paired comparison data: A review with emphasis on dependent data. Statistical Science, pages 412‚Äì433.

13.	Chang, H., Yu, F., Wang, J., Ashley, D., and Finkelstein, A. (2016). Automatic triage for a photo series. ACM Transactions on Graphics (TOG), 35(4):148.

14.	Desarkar, M. S., Saxena, R., and Sarkar, S. (2012). Preference relation based matrix factorization for recommender systems. In International conference on user modeling, adaptation, and personalization, pages 63‚Äì75. Springer.

15.	Doughty, H., Damen, D., and Mayol-Cuevas, W. (2018). Who‚Äôs better? who‚Äôs best? Pairwise deep ranking for skill determination. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6057‚Äì6066.

16.	Dubey, A., Naik, N., Parikh, D., Raskar, R., and Hidalgo, C. A. (2016). Deep learning the city: Quantifying urban perception at a global scale. In European Conference on Computer Vision, pages 196‚Äì212. Springer.

17.	Fligner, M. A. and Verducci, J. S. (1993). Probability models and statistical analyses for ranking data, volume 80. Springer.

18.	Golik, P., Doetsch, P., and Ney, H. (2013). Cross-entropy vs. squared error training: a theoretical and experimental comparison. In Interspeech, volume 13, pages 1756‚Äì1760.

19.	Guiver, J. and Snelson, E. (2009). Bayesian inference for plackett-luce ranking models. In Proceedings of the 26th Annual International Conference on Machine Learning, pages 377‚Äì384. ACM.

20.	Guo, Yuan, Peng Tian, Jayashree Kalpathy-Cramer, Susan Ostmo, J. Peter Campbell, Michael F. Chiang, Deniz Erdoƒümu≈ü, Jennifer G. Dy, and Stratis Ioannidis. "Experimental Design under the Bradley-Terry Model." In IJCAI, pp. 2198-2204. 2018.

21.	Guo, Yuan, Jennifer Dy, Deniz Erdoƒümu≈ü, Jayashree Kalpathy-Cramer, Susan Ostmo, J. Peter Campbell, Michael F. Chiang, and Stratis Ioannidis. "Accelerated experimental design for pairwise comparisons." Proceedings of the 2019 SIAM International Conference on Data Mining. Society for Industrial and Applied Mathematics, 2019.

22.	Guo, Yuan, Jennifer Dy, Deniz Erdoƒümu≈ü, Jayashree Kalpathy-Cramer, Susan Ostmo, J. Peter Campbell, Michael F. Chiang, and Stratis Ioannidis. "Variational inference from ranked samples with features." In Asian Conference on Machine Learning, pp. 599-614. PMLR, 2019.

23.	Hajek, B., Oh, S., and Xu, J. (2014). Minimax-optimal inference from partial rankings. In Advances in Neural Information Processing Systems, pages 1475‚Äì1483.

24.	Hunter, D. R. (2004). MM algorithms for generalized bradley-terry models. The Annals of Statistics, 32(1):384‚Äì406.

25.	T. Jaakkola and M. Jordan. A variational approach to bayesian logistic regression models and their extensions. In Sixth International Workshop on Artificial Intelligence and Statistics, 1997.

26.	Joachims, T. (2002). Optimizing search engines using clickthrough data. In Proceedings of the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 133‚Äì142. ACM.

27.	Kalpathy-Cramer, J., Campbell, J. P., Erdoƒümu≈ü, D., Tian, P., Kedarisetti, D., Moleta, C., Reynolds, J. D., Hutcheson, K., Shapiro, M. J., and Repka, M. X. (2016). Plus disease in retinopathy of prematurity: Improving diagnosis by ranking disease severity and using quantitative image analysis. Ophthalmology, 123(11):2345‚Äì2351.

28.	Khetan, A. and Oh, S. (2016). Computational and statistical tradeoffs in learning to rank. In Advances in Neural Information Processing Systems, pages 739‚Äì747.

29.	Koren, Y. and Sill, J. (2011). Ordrec: an ordinal model for predicting personalized item rating distributions. In Proceedings of the fifth ACM Conference on Recommender Systems, pages 117‚Äì124. ACM.

30.	Lei, Q., Zhong, K., and Dhillon, I. S. (2016). Coordinate-wise power method. In Advances in Neural Information Processing Systems, pages 2064‚Äì2072.

31.	Lu, T. and Boutilier, C. (2011). Learning mallows models with pairwise preferences. In Proceedings of the 28th International Conference on Machine Learning (icml-11), pages 145‚Äì152.

32.	Mao, C., Pananjady, A., and Wainwright, M. J. (2018a). Breaking the 1/n1/2 barrier: Faster rates for permutation-based models in polynomial time. In Conference On Learning Theory, pages 2037‚Äì2042.

33.	Mao, C., Weed, J., and Rigollet, P. (2018b). Minimax rates and efficient algorithms for noisy sorting. In Algorithmic Learning Theory, pages 821‚Äì847. PMLR.

34.	Marden, J. I. (2014). Analyzing and modeling rank data. Chapman and Hall/CRC.

35.	Maystre, L. and Grossglauser, M. (2015). Fast and accurate inference of plackett-luce models. In Advances in Neural Information Processing Systems, pages 172‚Äì180.

36.	M. Minoux. Accelerated greedy algorithms for maximizing submodular set functions. In Optimization techniques. 1978.

37.	C. Moleta, J. P. Campbell, J. Kalpathy-Cramer, R. P. Chan, S. Ostmo, K. Jonas, M. F. Chiang, I. . I. in ROP Research Consortium et al., ‚ÄúPlus disease in retinopathy of prematurity: Diagnostic trends in 2016 vs. 2007,‚Äù American Journal of Ophthalmology, 2017.

38.	Mosteller, F. (2006). Remarks on the method of paired comparisons: I. the least squares solution assuming equal standard deviations and equal correlations. In Selected Papers of Frederick Mosteller, pages 157‚Äì162. Springer.

39.	Negahban, S., Oh, S., and Shah, D. (2012). Iterative ranking from pair-wise comparisons. In Advances in Neural Information Processing Systems, pages 2474‚Äì2482.

40.	G. L. Nemhauser, L. A. Wolsey, and M. L Fisher. An analysis of approximations for maximizing submodular set functions. Mathematical Programming, 1978

41.	Pahikkala, T., Tsivtsivadze, E., Airola, A., J√§rvinen, J., and Boberg, J. (2009). An efficient algorithm for learning to rank from preference graphs. Machine Learning, 75(1):129‚Äì165.

42.	S. Park and S. Choi. Bayesian aggregation of binary classifiers. In ICDM, 2010.

43.	Plackett, R. L. (1975). The analysis of permutations. Applied Statistics, pages 193‚Äì202.

44.	Rajkumar, A. and Agarwal, S. (2014). A statistical convergence perspective of algorithms for rank aggregation from pairwise data. In International Conference on Machine Learning, pages 118‚Äì126.

45.	Schultz, M. and Joachims, T. (2004). Learning a distance metric from relative comparisons. In Advances in Neural Information Processing Systems, pages 41‚Äì48.

46.	Shah, N., Balakrishnan, S., Guntuboyina, A., and Wainwright, M. (2016a). Stochastically transitive models for pairwise comparisons: Statistical and computational issues. In International Conference on Machine Learning, pages 11‚Äì20.

47.	Shah, N. B., Balakrishnan, S., Bradley, J., Parekh, A., Ramchandran, K., and Wainwright, M. J. (2016b). Estimation from pairwise comparisons: Sharp minimax bounds with topology dependence. The Journal of Machine Learning Research, 17(1):2049‚Äì2095.

48.	Soufiani, H. A., Chen, W., Parkes, D. C., and Xia, L. (2013). Generalized method-of-moments for rank aggregation. In Advances in Neural Information Processing Systems, pages 2706‚Äì2714.

49.	Tian, P., Guo, Y., Kalpathy-Cramer, J., Ostmo, S., Campbell, J. P., Chiang, M. F., Dy, J., Erdoƒümu≈ü, D., and Ioannidis, S. (2019). A severity score for retinopathy of prematurity. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 1809‚Äì1819. ACM.

50.	Vojnovic, M. and Yun, S. (2016). Parameter estimation for generalized thurstone choice models. In International Conference on Machine Learning, pages 498‚Äì506.

51.	Vojnovic, Milan, Se-Young Yun, and Kaifang Zhou. "Convergence Rates of Gradient Descent and MM Algorithms for Bradley-Terry Models." International Conference on Artificial Intelligence and Statistics. PMLR, 2020.

52.	Wauthier, F., Jordan, M., and Jojic, N. (2013). Efficient ranking from pairwise comparisons. In International Conference on Machine Learning, pages 109‚Äì117.

53.	Yƒ±ldƒ±z, ƒ∞., Dy, J., Erdoƒümu≈ü, D., Kalpathy-Cramer, J., Ostmo, S., Campbell, J. P., Chiang, M. F., and Ioannidis, S. (2020). Fast and accurate ranking regression. In International Conference on Artificial Intelligence and Statistics (AISTATS).

54.	Yƒ±ldƒ±z, ƒ∞., Dy, J., Erdoƒümu≈ü, D., Ostmo, S., Campbell, J. P., Chiang, M. F., and Ioannidis, S. (2021). Deep spectral ranking. In International Conference on Artificial Intelligence and Statistics (AISTATS)

55.	Yƒ±ldƒ±z, ƒ∞., Tian, P., Dy, J., Erdoƒümu≈ü, D., Brown, J., Kalpathy-Cramer, J., Ostmo, S., Campbell, J. P., Chiang, M. F., and Ioannidis, S. (2019). Classification and comparison via neural networks. Neural Networks.

56.	Zheng, Y., Zhang, L., Xie, X., and Ma, W.-Y. (2009). Mining interesting locations and travel sequences from gps trajectories. In Proceedings of the 18th International Conference on World Wide Web, pages 791‚Äì800. ACM.


